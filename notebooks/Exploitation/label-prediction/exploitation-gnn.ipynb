{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation - Label Prediction\n",
    "\n",
    "In the exploitation task, we aim to predict the type of edges (transport routes). We start with Hand crafted features, followed by node embedding and finally use GNNs. In this notebook, we will work using GNNs.\n",
    "\n",
    "## Task - 2\n",
    "\n",
    "In the second task, we predict the edge labels between the given nodes. \n",
    "\n",
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Union, List, Dict, Literal, Tuple\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "from node2vec import Node2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths for input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_data_folder_path = pathlib.Path(\"./../../../data\")\n",
    "transport_data_path = rel_data_folder_path.joinpath('transport_data')\n",
    "city_network = rel_data_folder_path.joinpath('network_graphs')\n",
    "city_network_graphs = city_network.joinpath('graphs')\n",
    "city_network_graphs_dir = city_network_graphs.joinpath('directed_graphs')\n",
    "city_network_bones = city_network.joinpath('nodes-edges')\n",
    "\n",
    "checkpoints_folder_path = rel_data_folder_path.joinpath(\"checkpoints\")\n",
    "city_network_graphs_dir_label_pred_node2vec = checkpoints_folder_path.joinpath('node2vec-label-pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define enum for route types\n",
    "class RouteType(Enum):\n",
    "    tram, subway, rail, bus, ferry, cablecar, gondola = range(7)\n",
    "\n",
    "def load_city_graphs(city_name: str, graphs_folder: pathlib.Path) -> Dict[str, Union[float, List[List[int]], nx.Graph]]:\n",
    "    with open(graphs_folder.joinpath(city_name.lower() + '.gpickle'), 'rb') as f:\n",
    "        city_graph = pickle.load(f)\n",
    "    return city_graph\n",
    "\n",
    "def load_all_cities_graphs(cities: List[str], graphs_folder: pathlib.Path) -> Dict[str, Dict[str, Union[float, List[List[int]], nx.Graph]]]:\n",
    "    return {city: load_city_graphs(city, graphs_folder) for city in cities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric as pyg\n",
    "from torch_geometric.data import Data\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Graph neural network module. It comprises of a series of `pyg.nn.GraphConv` Graph convolutional layers\n",
    "followed by the pooling layer that uses addition based reduction.\n",
    "\"\"\"\n",
    "class GNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Initialize the GNN model layers.\n",
    "    Args:\n",
    "        num_node_features: int -> Dimension of the edge-feature vector.\n",
    "        num_classes: int -> Number of classes to consider for the final linear layer's output, the output vector dimension\n",
    "\n",
    "    Returns:\n",
    "        nn.Module -> GNN model\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_node_features: int, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = pyg.nn.GraphConv(num_node_features, 32)\n",
    "        self.conv2 = pyg.nn.GraphConv(32, 64)\n",
    "\n",
    "        self.linear1 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    \"\"\"\n",
    "    Forward pass function for the GNN model.\n",
    "    Args:\n",
    "        x -> Node feature matrix\n",
    "        edge_index -> connectivity tensor\n",
    "        batch -> batch vector that assigns a node to a specific data sample.\n",
    "    \"\"\"\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x_edges = x[edge_index]\n",
    "        x_edges = torch.cat((x_edges[0], x_edges[1]), dim=1)\n",
    "        return self.linear1(x_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\"\"\"\n",
    "Method to evaluate the trained model against test data\n",
    "to compute the test loss as well as the F1-score metrics.\n",
    "\n",
    "Args:\n",
    "    model: nn.Module -> GNN model\n",
    "    loss_fcn: torch_geometric.nn.loss -> Loss function that was used for training.\n",
    "    device: str -> 'cpu' or 'cuda' to mention the device to use for evaluation.\n",
    "    dataloader -> torch_geometric.loader.DataLoader -> Dataloader for test dataset\n",
    "\n",
    "Returns:\n",
    "    np.float64 -> Average F1 score on test dataset\n",
    "\"\"\"\n",
    "def evaluate(model, loss_fcn, device, dataloader):\n",
    "\n",
    "    score_list_batch = []\n",
    "\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        output = model(batch.x, batch.edge_index)\n",
    "        loss_test = loss_fcn(output, batch.y)\n",
    "        predict = np.where(output.detach().cpu().numpy() >= 0, 1, 0)\n",
    "        score = f1_score(batch.y.cpu().numpy(), predict, average=\"micro\")\n",
    "        score_list_batch.append(score)\n",
    "\n",
    "    return np.array(score_list_batch).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of train method for GNNs\n",
    "\n",
    "Args:\n",
    "    model: nn.Module -> Model to use for training and validation\n",
    "    loss_fcn: nn.CrossEntropyLoss() -> Cross entropy or any loss function for the training task\n",
    "    device: 'str' ['cpu' | 'cuda' ] -> Device to use for training\n",
    "    optimizer: torch.optim.<Optimizer> -> Optimizer\n",
    "    max_epocs: int -> Number of epochs (max) to run training for.\n",
    "    train_dataloader: torch_geometric.loader.Dataloader -> Dataloader for training samples\n",
    "    val_dataloader: torch_geometric.loader.Dataloader -> Dataloader for test samples.  \n",
    "\"\"\"\n",
    "def train(model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader):\n",
    "    epoch_list = []\n",
    "    scores_list = []\n",
    "\n",
    "    # loop over epochs\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        # loop over batches\n",
    "        for i, train_batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            train_batch_device = train_batch.to(device)\n",
    "            # logits is the output of the model\n",
    "            logits = model(train_batch_device.x, train_batch_device.edge_index)\n",
    "            # compute the loss\n",
    "            loss = loss_fcn(logits, train_batch_device.y)\n",
    "            # optimizer step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        loss_data = np.array(losses).mean()\n",
    "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch + 1, loss_data))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            # evaluate the model on the validation set\n",
    "            # computes the f1-score (see next function)\n",
    "            score = evaluate(model, loss_fcn, device, val_dataloader)\n",
    "            print(\"F1-Score: {:.4f}\".format(score))\n",
    "            scores_list.append(score)\n",
    "            epoch_list.append(epoch)\n",
    "\n",
    "    return epoch_list, scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only consider the full route type as it has the data regarding\n",
    "# the target values for all the various types of transport.\n",
    "route_type = 'full'\n",
    "num_targets = len(RouteType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mGenerates target labels for the edges in a graph\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39mArgsL\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_edge_targets\u001b[39m(graph: nx\u001b[39m.\u001b[39mGraph, num_targets: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m      7\u001b[0m     targets \u001b[39m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m     edges_unique \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generates target labels for the edges in a graph\n",
    "\n",
    "Args:\n",
    "    graph: nx.MultiDiGraph -> Mulit directed-graph with parallel edges corresponding to different target labels.\n",
    "    num_targets: int -> Number of target (distinct) labels.\n",
    "\n",
    "Returns:\n",
    "    np.ndarray: [num_edges, num_targets] -> Target label mask for each edge of the graph\n",
    "\"\"\"\n",
    "def generate_edge_targets(graph: nx.Graph, num_targets: int) -> np.ndarray:\n",
    "    targets = []\n",
    "    edges_unique = []\n",
    "    for edge in graph.edges():\n",
    "        if edge not in edges_unique:\n",
    "            edges_unique.append(edge)\n",
    "    \n",
    "    for node_1, node_2 in edges_unique:\n",
    "        target = np.zeros(num_targets)\n",
    "        edge_data = graph.get_edge_data(node_1, node_2)\n",
    "        for edge_attr in edge_data.values():\n",
    "            target[edge_attr['route_type']] = 1\n",
    "        targets.append(target)\n",
    "\n",
    "    return np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Node2Vec_node_feature_extraction(graph: nx.Graph, num_features: int, p: float, q: float, seed: int) -> Dict[float, np.ndarray]:\n",
    "    ''' \n",
    "    INPUT:\n",
    "    graph: the graph\n",
    "    num_features: dimension of node2vec embeddings, int\n",
    "    p: float\n",
    "    q: float\n",
    "    seed: please always set to 0\n",
    "\n",
    "    OUTPUT:\n",
    "    features: feature matrix of dimensions (N, D) (N: number of samples; D: dimension of Node2Vec embeddings) \n",
    "    '''\n",
    "     \n",
    "    node2vec_ = Node2Vec(graph, dimensions=num_features, p=p, q=q, seed=seed)\n",
    "    model = node2vec_.fit()\n",
    "    features_dict = {node: model.wv[idx] for idx, node in enumerate(graph.nodes())}\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]/tmp/ipykernel_603/119258313.py:45: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj_matrix = nx.adjacency_matrix(route_graph).todense()\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Split the graphs for train and test and create the dataset and dataloaders for each\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "import torch\n",
    "\n",
    "# Create a random number generator\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(seed=2106)\n",
    "\n",
    "test_split = 0.3\n",
    "device = 'cpu'\n",
    "\n",
    "cities = sorted([x.stem for x in city_network_graphs_dir.glob('*.gpickle')])\n",
    "city_graphs_dir = load_all_cities_graphs(cities, city_network_graphs_dir)\n",
    "city_routes = {}\n",
    "dataset_train = []\n",
    "dataset_test  = []\n",
    "\n",
    "for city, city_graphs in city_graphs_dir.items():\n",
    "    routes_ = []\n",
    "    for route_type, route_graph in city_graphs.items():\n",
    "        if ((route_type in ['full', 'cablecar']) or (route_graph is None)):\n",
    "            continue\n",
    "        routes_.append(RouteType[route_type].value)\n",
    "    if len(routes_) > 1:\n",
    "        city_routes[city] = routes_\n",
    "\n",
    "for city, city_graphs in tqdm(city_graphs_dir.items()):\n",
    "    if city.lower() == 'sydney':\n",
    "        continue\n",
    "    city_graph_scores = {}\n",
    "    if city in city_routes:\n",
    "        route_graph = city_graphs[route_type]\n",
    "        num_targets = len(RouteType)\n",
    "\n",
    "        targets_edge_all = generate_edge_targets(route_graph, num_targets=num_targets)\n",
    "        targets_edge = np.array(targets_edge_all)\n",
    "        num_edges = targets_edge.shape[0]\n",
    "        torch_data = from_networkx(route_graph)\n",
    "\n",
    "        # Take the complement of the `route_graph` and augment equal number of edges as original graph to the dataset.\n",
    "        # As this is computationally expensive, we find some of these edges from its adjacency matrix\n",
    "        adj_matrix = nx.adjacency_matrix(route_graph).todense()\n",
    "        complement_edges = set()\n",
    "        for row_idx in range(adj_matrix.shape[0]):\n",
    "            for col_idx in range(adj_matrix.shape[0]):\n",
    "                if row_idx == col_idx:\n",
    "                    continue\n",
    "                if (row_idx, col_idx) not in complement_edges:\n",
    "                    complement_edges.add((row_idx, col_idx))\n",
    "                if len(complement_edges) == num_edges:\n",
    "                    break\n",
    "        complement_edges = list(complement_edges)\n",
    "        complement_edges_row = [edge[0] for edge in complement_edges]\n",
    "        complement_edges_col = [edge[1] for edge in complement_edges]\n",
    "        del complement_edges\n",
    "\n",
    "        targets_complement = np.zeros(shape=(targets_edge.shape))\n",
    "        complement_edge_index = torch.tensor([np.array(complement_edges_row), np.array(complement_edges_col)], dtype=torch.long)\n",
    "        del complement_edges_row\n",
    "        del complement_edges_col\n",
    "\n",
    "        # Initial input features as node2vec features\n",
    "        node_features = Node2Vec_node_feature_extraction(route_graph, num_features=10, p=1.00, q=5.00, seed=0)\n",
    "        node_features = np.array([ node_features[node] for node in route_graph.nodes() ])\n",
    "\n",
    "        dataset = Data(x=node_features, edge_index=torch.cat((torch_data.edge_index, complement_edge_index), dim=1), y=torch.cat((targets_edge, targets_complement)))\n",
    "        dataset = dataset.to(device)\n",
    "\n",
    "        num_edges_dataset = 2 * num_edges\n",
    "\n",
    "        # Split train and test data\n",
    "        train_mask = np.zeros(num_edges_dataset, dtype=int)\n",
    "        train_mask[:int((1.000 - test_split) * num_edges_dataset)] = True\n",
    "        rng.shuffle(train_mask)\n",
    "        test_mask = ~train_mask\n",
    "\n",
    "        train_mask = torch.tensor(train_mask, device=device).bool()\n",
    "\n",
    "        train_data = Data(x=dataset.x, edge_index=dataset.edge_index[:, train_mask], y=dataset.y[train_mask])\n",
    "        test_data  = Data(x=dataset.x, edge_index=dataset.edge_index[:, test_mask], y=dataset.y[test_mask])\n",
    "\n",
    "        dataset_train.append(train_data)\n",
    "        dataset_test.append(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, shuffle=True)\n",
    "dataloader_test  = DataLoader(dataset_test, shuffle=True)\n",
    "\n",
    "# GNN model for label prediction in edges\n",
    "model = GNN(10, num_targets)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train\n",
    "train(model, loss_fn, device=device, optimizer=optimizer, max_epochs=50, train_dataloader=dataloader_train, val_dataloader=dataloader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
