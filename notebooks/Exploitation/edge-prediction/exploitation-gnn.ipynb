{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation - 1\n",
    "\n",
    "In the exploitation task, we aim to predict the edges and the type of edges (transport routes). We start with Hand crafted features, followed by node embedding and finally use GNNs. In this notebook, we will explore the use of Graph Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from node2vec import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_data_folder_path = pathlib.Path(\"./../../../data\")\n",
    "transport_data_path = rel_data_folder_path.joinpath('transport_data')\n",
    "city_network = rel_data_folder_path.joinpath('network_graphs')\n",
    "city_network_graphs = city_network.joinpath('graphs')\n",
    "city_network_graphs_dir = city_network_graphs.joinpath('directed_graphs')\n",
    "city_network_bones = city_network.joinpath('nodes-edges')\n",
    "\n",
    "checkpoints_folder_path = rel_data_folder_path.joinpath(\"checkpoints\")\n",
    "city_network_graphs_dir_edge_pred_gnns = checkpoints_folder_path.joinpath('gnn-single-class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to load the dumped graphs\n",
    "class RouteType(Enum):\n",
    "    tram, subway, rail, bus, ferry, cablecar, gondola = range(7)\n",
    "\n",
    "def load_city_graphs(city_name, graphs_folder):\n",
    "    with open(graphs_folder.joinpath(city_name.lower() + '.gpickle'), 'rb') as f:\n",
    "        city_graph = pickle.load(f)\n",
    "    return city_graph\n",
    "\n",
    "def load_all_cities_graphs(cities: list[str], graphs_folder: pathlib.Path):\n",
    "    return {city: load_city_graphs(city, graphs_folder) for city in cities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric as pyg\n",
    "from torch_geometric.data import Data\n",
    "from torch import nn\n",
    "\n",
    "\"\"\"\n",
    "Graph neural network module. It comprises of a series of `pyg.nn.GraphConv` Graph convolutional layers\n",
    "followed by the pooling layer that uses addition based reduction.\n",
    "\"\"\"\n",
    "class GNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Initialize the GNN model layers.\n",
    "    Args:\n",
    "        num_edge_features: int -> Dimension of the edge-feature vector.\n",
    "        num_classes: int -> Number of classes to consider for the final linear layer's output, the output vector dimension\n",
    "\n",
    "    Returns:\n",
    "        nn.Module -> GNN model\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_edge_features: int, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = pyg.nn.GraphConv(num_edge_features, 64)\n",
    "        self.conv2 = pyg.nn.GraphConv(64, 64)\n",
    "        self.conv3 = pyg.nn.GraphConv(64, 128)\n",
    "        self.conv4 = pyg.nn.GraphConv(128, 128)\n",
    "        self.conv5 = pyg.nn.GraphConv(128, 256)\n",
    "\n",
    "        self.linear1 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    \"\"\"\n",
    "    Forward pass function for the GNN model.\n",
    "    Args:\n",
    "        x -> Node feature matrix\n",
    "        edge_index -> connectivity tensor\n",
    "        batch -> batch vector that assigns a node to a specific data sample.\n",
    "    \"\"\"\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.conv4(x, edge_index).relu()\n",
    "        x = self.conv5(x, edge_index).relu()\n",
    "\n",
    "        x = pyg.nn.global_add_pool(x, batch)\n",
    "        return self.linear1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\"\"\"\n",
    "Method to evaluate the trained model against test data\n",
    "to compute the test loss as well as the F1-score metrics.\n",
    "\n",
    "Args:\n",
    "    model: nn.Module -> GNN model\n",
    "    loss_fcn: torch_geometric.nn.loss -> Loss function that was used for training.\n",
    "    device: str -> 'cpu' or 'cuda' to mention the device to use for evaluation.\n",
    "    dataloader -> torch_geometric.loader.DataLoader -> Dataloader for test dataset\n",
    "\n",
    "Returns:\n",
    "    np.float64 -> Average F1 score on test dataset\n",
    "\"\"\"\n",
    "def evaluate(model, loss_fcn, device, dataloader):\n",
    "\n",
    "    score_list_batch = []\n",
    "\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        output = model(batch.x, batch.edge_index)\n",
    "        loss_test = loss_fcn(output, batch.y)\n",
    "        predict = np.where(output.detach().cpu().numpy() >= 0, 1, 0)\n",
    "        score = f1_score(batch.y.cpu().numpy(), predict, average=\"micro\")\n",
    "        score_list_batch.append(score)\n",
    "\n",
    "    return np.array(score_list_batch).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader):\n",
    "    epoch_list = []\n",
    "    scores_list = []\n",
    "\n",
    "    # loop over epochs\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        # loop over batches\n",
    "        for i, train_batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            train_batch_device = train_batch.to(device)\n",
    "            # logits is the output of the model\n",
    "            logits = model(train_batch_device.x, train_batch_device.edge_index)\n",
    "            # compute the loss\n",
    "            loss = loss_fcn(logits, train_batch_device.y)\n",
    "            # optimizer step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        loss_data = np.array(losses).mean()\n",
    "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch + 1, loss_data))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            # evaluate the model on the validation set\n",
    "            # computes the f1-score (see next function)\n",
    "            score = evaluate(model, loss_fcn, device, val_dataloader)\n",
    "            print(\"F1-Score: {:.4f}\".format(score))\n",
    "            scores_list.append(score)\n",
    "            epoch_list.append(epoch)\n",
    "\n",
    "    return epoch_list, scores_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
