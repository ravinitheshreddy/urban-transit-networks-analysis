{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation - Label Prediction\n",
    "\n",
    "In the exploitation task, we aim to predict the type of edges (transport routes). We start with Hand crafted features, followed by node embedding and finally use GNNs. In this notebook, we will work using GNNs.\n",
    "\n",
    "## Task - 2\n",
    "\n",
    "In the second task, we predict the edge labels between the given nodes. \n",
    "\n",
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Union, List, Dict, Literal, Tuple\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "from node2vec import Node2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths for input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_data_folder_path = pathlib.Path(\"./../../../data\")\n",
    "transport_data_path = rel_data_folder_path.joinpath('transport_data')\n",
    "city_network = rel_data_folder_path.joinpath('network_graphs')\n",
    "city_network_graphs = city_network.joinpath('graphs')\n",
    "city_network_graphs_dir = city_network_graphs.joinpath('directed_graphs')\n",
    "city_network_bones = city_network.joinpath('nodes-edges')\n",
    "\n",
    "checkpoints_folder_path = rel_data_folder_path.joinpath(\"checkpoints\")\n",
    "city_network_graphs_dir_label_pred_node2vec = checkpoints_folder_path.joinpath('node2vec-label-pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define enum for route types\n",
    "class RouteType(Enum):\n",
    "    tram, subway, rail, bus, ferry, cablecar, gondola = range(7)\n",
    "\n",
    "def load_city_graphs(city_name: str, graphs_folder: pathlib.Path) -> Dict[str, Union[float, List[List[int]], nx.Graph]]:\n",
    "    with open(graphs_folder.joinpath(city_name.lower() + '.gpickle'), 'rb') as f:\n",
    "        city_graph = pickle.load(f)\n",
    "    return city_graph\n",
    "\n",
    "def load_all_cities_graphs(cities: List[str], graphs_folder: pathlib.Path) -> Dict[str, Dict[str, Union[float, List[List[int]], nx.Graph]]]:\n",
    "    return {city: load_city_graphs(city, graphs_folder) for city in cities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric as pyg\n",
    "from torch_geometric.data import Data\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Graph neural network module. It comprises of a series of `pyg.nn.GraphConv` Graph convolutional layers\n",
    "followed by the pooling layer that uses addition based reduction.\n",
    "\"\"\"\n",
    "class GNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Initialize the GNN model layers.\n",
    "    Args:\n",
    "        num_node_features: int -> Dimension of the edge-feature vector.\n",
    "        num_classes: int -> Number of classes to consider for the final linear layer's output, the output vector dimension\n",
    "\n",
    "    Returns:\n",
    "        nn.Module -> GNN model\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_node_features: int, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = pyg.nn.GraphConv(num_node_features, 16)\n",
    "\n",
    "        self.linear1 = nn.Linear(32, num_classes)\n",
    "    \n",
    "    \"\"\"\n",
    "    Forward pass function for the GNN model.\n",
    "    Args:\n",
    "        x -> Node feature matrix\n",
    "        edge_index -> connectivity tensor\n",
    "        batch -> batch vector that assigns a node to a specific data sample.\n",
    "    \"\"\"\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x_edges = x[edge_index]\n",
    "        x_edges = torch.cat((x_edges[0], x_edges[1]), dim=1)\n",
    "        return self.linear1(x_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\"\"\"\n",
    "Method to evaluate the trained model against test data\n",
    "to compute the test loss as well as the F1-score metrics.\n",
    "\n",
    "Args:\n",
    "    model: nn.Module -> GNN model\n",
    "    loss_fcn: torch_geometric.nn.loss -> Loss function that was used for training.\n",
    "    device: str -> 'cpu' or 'cuda' to mention the device to use for evaluation.\n",
    "    dataloader -> torch_geometric.loader.DataLoader -> Dataloader for test dataset\n",
    "\n",
    "Returns:\n",
    "    np.float64 -> Average F1 score on test dataset\n",
    "\"\"\"\n",
    "def evaluate(model, loss_fcn, device, dataloader):\n",
    "\n",
    "    score_list_batch = []\n",
    "\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        output = torch.sigmoid(model(batch.x, batch.edge_index))\n",
    "\n",
    "        loss_test = loss_fcn(output, batch.y)\n",
    "        print(\"Test loss {}\".format(loss_test.mean()))\n",
    "        predict = np.where(output.detach().cpu().numpy() >= 0.5, 1, 0)\n",
    "        labels = batch.y.cpu().numpy()\n",
    "\n",
    "        score = f1_score(labels, predict, average=\"weighted\")\n",
    "        score_list_batch.append(score)\n",
    "\n",
    "    return np.array(score_list_batch).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of train method for GNNs\n",
    "\n",
    "Args:\n",
    "    model: nn.Module -> Model to use for training and validation\n",
    "    loss_fcn: nn.BCELoss() -> Binary cross entropy or any loss function for the training task\n",
    "    device: 'str' ['cpu' | 'cuda' ] -> Device to use for training\n",
    "    optimizer: torch.optim.<Optimizer> -> Optimizer\n",
    "    max_epocs: int -> Number of epochs (max) to run training for.\n",
    "    train_dataloader: torch_geometric.loader.Dataloader -> Dataloader for training samples  \n",
    "\"\"\"\n",
    "def train(model, loss_fcn, device, optimizer, max_epochs, train_dataloader):\n",
    "    epoch_list = []\n",
    "    scores_list = []\n",
    "    print(\"Training the model\")\n",
    "\n",
    "    # loop over epochs\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        # loop over batches\n",
    "        for i, train_batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            train_batch_device = train_batch.to(device)\n",
    "            # logits is the output of the model\n",
    "            logits = model(train_batch_device.x, train_batch_device.edge_index)\n",
    "            # compute the loss\n",
    "            loss = loss_fcn(logits, train_batch_device.y)\n",
    "            # optimizer step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        loss_data = np.array(losses).mean()\n",
    "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch + 1, loss_data))\n",
    "\n",
    "    return epoch_list, scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only consider the full route type as it has the data regarding\n",
    "# the target values for all the various types of transport.\n",
    "route_type = 'full'\n",
    "num_targets = len(RouteType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates target labels for the edges in a graph\n",
    "\n",
    "Args:\n",
    "    graph: nx.MultiDiGraph -> Mulit directed-graph with parallel edges corresponding to different target labels.\n",
    "    num_targets: int -> Number of target (distinct) labels.\n",
    "\n",
    "Returns:\n",
    "    np.ndarray: [num_edges, num_targets] -> Target label mask for each edge of the graph\n",
    "\"\"\"\n",
    "def generate_edge_targets(graph: nx.Graph, num_targets: int) -> np.ndarray:\n",
    "    targets = []\n",
    "    edges_unique = []\n",
    "    for edge in graph.edges():\n",
    "        if edge not in edges_unique:\n",
    "            edges_unique.append(edge)\n",
    "\n",
    "    for node_1, node_2 in edges_unique:\n",
    "        target = np.zeros(num_targets)\n",
    "        edge_data = graph.get_edge_data(node_1, node_2)\n",
    "        for edge_attr in edge_data.values():\n",
    "            target[edge_attr['route_type']] = 1\n",
    "        targets.append(target)\n",
    "\n",
    "    return np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Node2Vec_node_feature_extraction(graph: nx.Graph, num_features: int, p: float, q: float, seed: int) -> Dict[float, np.ndarray]:\n",
    "    ''' \n",
    "    INPUT:\n",
    "    graph: the graph\n",
    "    num_features: dimension of node2vec embeddings, int\n",
    "    p: float\n",
    "    q: float\n",
    "    seed: please always set to 0\n",
    "\n",
    "    OUTPUT:\n",
    "    features: feature matrix of dimensions (N, D) (N: number of samples; D: dimension of Node2Vec embeddings) \n",
    "    '''\n",
    "\n",
    "    node2vec_ = Node2Vec(graph, dimensions=num_features, p=p, q=q, seed=seed)\n",
    "    model = node2vec_.fit()\n",
    "    features_dict = {node: model.wv[idx] for idx, node in enumerate(graph.nodes())}\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "def get_torch_data(graph: nx.MultiDiGraph, num_targets: int) -> torch_geometric.data.Data:\n",
    "    graph_nodes = list(graph.nodes())\n",
    "    node_features = Node2Vec_node_feature_extraction(graph, num_features=25, p=1.00, q=5.00, seed=0)\n",
    "    node_features = torch.tensor(np.array([ node_features[node] for node in graph.nodes() ]))\n",
    "\n",
    "    # Labels for the graph edges\n",
    "    targets = []\n",
    "    edges_unique = []\n",
    "    node_to_idx_map = {}\n",
    "\n",
    "    for idx, node in enumerate(graph_nodes):\n",
    "        node_to_idx_map[int(node)] = idx\n",
    "\n",
    "    for edge in graph.edges():\n",
    "        if edge not in edges_unique:\n",
    "            edges_unique.append((edge[0], edge[1]))\n",
    "\n",
    "    # Construct edge index (COO type) corresponding to a complete directed graph\n",
    "    print(\"Number of unique edges: {}\".format(len(edges_unique)))\n",
    "    print(list(edges_unique)[0])\n",
    "    num_edges_graph = len(edges_unique)\n",
    "    coo_vector = np.zeros(shape=(2, 2 * num_edges_graph), dtype=int)\n",
    "    coo_vector = torch.tensor(coo_vector, dtype=torch.long)\n",
    "    idx = 0\n",
    "    targets = []\n",
    "    # print(\"Number of nodes: {}\".format(num_nodes))\n",
    "    # print(\"Max node value {}, min node value {}\".format(min(graph_nodes), max(graph_nodes)))\n",
    "    # Add data entries for edges and non-edges equally\n",
    "    \n",
    "    num_non_edges = 0\n",
    "    num_edges_visited = 0\n",
    "    for node_1 in graph_nodes:\n",
    "        for node_2 in graph_nodes:\n",
    "            if node_1 == node_2:\n",
    "                continue\n",
    "            \n",
    "            target = np.zeros(num_targets, dtype=np.float32)\n",
    "            if (node_1, node_2) in edges_unique:\n",
    "                coo_vector[0][idx] = node_to_idx_map[node_1]\n",
    "                coo_vector[1][idx] = node_to_idx_map[node_2]\n",
    "                idx = idx + 1\n",
    "                edge_data = graph.get_edge_data(node_1, node_2)\n",
    "                for edge_attr in edge_data.values():\n",
    "                    target[edge_attr['route_type']] = 1\n",
    "                targets.append(target)\n",
    "                num_edges_visited = num_edges_visited + 1\n",
    "            else:\n",
    "                if num_non_edges < num_edges_graph:\n",
    "                    coo_vector[0][idx] = node_to_idx_map[node_1]\n",
    "                    coo_vector[1][idx] = node_to_idx_map[node_2]\n",
    "                    idx = idx + 1\n",
    "                    num_non_edges = num_non_edges + 1\n",
    "                    targets.append(target)\n",
    "            if (num_edges_visited == num_edges_graph) and (num_non_edges == num_edges_graph):\n",
    "                print(\"[breaking] Number of targets {}, number of edges visited {}, number of non_edges {}\".format(len(targets), num_edges_visited, num_non_edges))\n",
    "                break\n",
    "            \n",
    "    targets = np.array(targets)\n",
    "    print(\"Number of targets {}, number of edges visited {}, number of non_edges {}\".format(targets.shape, num_edges_visited, num_non_edges))\n",
    "    return Data(x=node_features, edge_index=coo_vector, y=torch.tensor(np.array(targets), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adelaide': [0, 2, 3], 'berlin': [0, 1, 2, 3, 4], 'bordeaux': [0, 3, 4], 'brisbane': [2, 3, 4], 'canberra': [2, 3], 'dublin': [0, 2, 3], 'grenoble': [0, 3], 'helsinki': [0, 1, 2, 3, 4], 'lisbon': [1, 2, 3, 4], 'luxembourg': [2, 3], 'melbourne': [0, 2, 3], 'nantes': [0, 3], 'paris': [0, 1, 2, 3], 'prague': [0, 1, 3, 4], 'rennes': [1, 3], 'rome': [0, 1, 2, 3], 'sydney': [0, 2, 3, 4], 'toulouse': [0, 1, 3], 'venice': [0, 3, 4]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dbbb4cc6394e368f1e28a2060321df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/697 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [00:01<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique edges: 1546\n",
      "(1.0, 1089.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nitish/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "100%|██████████| 25/25 [00:18<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[breaking] Number of targets 3092, number of edges visited 1546, number of non_edges 1546\n",
      "Number of targets (3092, 7), number of edges visited 1546, number of non_edges 1546\n",
      "Training the model\n",
      "Epoch 00001 | Loss: 0.7680\n",
      "Epoch 00002 | Loss: 0.7664\n",
      "Epoch 00003 | Loss: 0.7649\n",
      "Epoch 00004 | Loss: 0.7634\n",
      "Epoch 00005 | Loss: 0.7618\n",
      "Epoch 00006 | Loss: 0.7603\n",
      "Epoch 00007 | Loss: 0.7588\n",
      "Epoch 00008 | Loss: 0.7573\n",
      "Epoch 00009 | Loss: 0.7558\n",
      "Epoch 00010 | Loss: 0.7544\n",
      "Epoch 00011 | Loss: 0.7529\n",
      "Epoch 00012 | Loss: 0.7514\n",
      "Epoch 00013 | Loss: 0.7499\n",
      "Epoch 00014 | Loss: 0.7485\n",
      "Epoch 00015 | Loss: 0.7470\n",
      "Epoch 00016 | Loss: 0.7456\n",
      "Epoch 00017 | Loss: 0.7442\n",
      "Epoch 00018 | Loss: 0.7427\n",
      "Epoch 00019 | Loss: 0.7413\n",
      "Epoch 00020 | Loss: 0.7399\n",
      "Epoch 00021 | Loss: 0.7385\n",
      "Epoch 00022 | Loss: 0.7371\n",
      "Epoch 00023 | Loss: 0.7357\n",
      "Epoch 00024 | Loss: 0.7343\n",
      "Epoch 00025 | Loss: 0.7329\n",
      "Epoch 00026 | Loss: 0.7315\n",
      "Epoch 00027 | Loss: 0.7302\n",
      "Epoch 00028 | Loss: 0.7288\n",
      "Epoch 00029 | Loss: 0.7274\n",
      "Epoch 00030 | Loss: 0.7261\n",
      "Epoch 00031 | Loss: 0.7247\n",
      "Epoch 00032 | Loss: 0.7234\n",
      "Epoch 00033 | Loss: 0.7220\n",
      "Epoch 00034 | Loss: 0.7207\n",
      "Epoch 00035 | Loss: 0.7194\n",
      "Epoch 00036 | Loss: 0.7180\n",
      "Epoch 00037 | Loss: 0.7167\n",
      "Epoch 00038 | Loss: 0.7154\n",
      "Epoch 00039 | Loss: 0.7141\n",
      "Epoch 00040 | Loss: 0.7128\n",
      "Epoch 00041 | Loss: 0.7115\n",
      "Epoch 00042 | Loss: 0.7102\n",
      "Epoch 00043 | Loss: 0.7089\n",
      "Epoch 00044 | Loss: 0.7076\n",
      "Epoch 00045 | Loss: 0.7064\n",
      "Epoch 00046 | Loss: 0.7051\n",
      "Epoch 00047 | Loss: 0.7038\n",
      "Epoch 00048 | Loss: 0.7026\n",
      "Epoch 00049 | Loss: 0.7013\n",
      "Epoch 00050 | Loss: 0.7000\n",
      "Training complete for city grenoble\n",
      "Evaluating the model on testing data\n",
      "Test loss 0.9046412110328674\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the graphs for train and test and create the dataset and dataloaders for each\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "import torch\n",
    "\n",
    "# Create a random number generator\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(seed=2106)\n",
    "\n",
    "test_split = 0.3\n",
    "device = 'cpu'\n",
    "\n",
    "cities = sorted([x.stem for x in city_network_graphs_dir.glob('*.gpickle')])\n",
    "city_graphs_dir = load_all_cities_graphs(cities, city_network_graphs_dir)\n",
    "city_routes = {}\n",
    "dataset_train = []\n",
    "dataset_test  = []\n",
    "\n",
    "for city, city_graphs in city_graphs_dir.items():\n",
    "    routes_ = []\n",
    "    for route_type, route_graph in city_graphs.items():\n",
    "        if ((route_type in ['full', 'cablecar']) or (route_graph is None)):\n",
    "            continue\n",
    "        routes_.append(RouteType[route_type].value)\n",
    "    if len(routes_) > 1:\n",
    "        city_routes[city] = routes_\n",
    "print(city_routes)\n",
    "\n",
    "for city, city_graphs in tqdm(city_graphs_dir.items()):\n",
    "    if city.lower() == 'sydney':\n",
    "        continue\n",
    "\n",
    "    if city.lower() != 'grenoble':\n",
    "        continue\n",
    "\n",
    "    city_graph_scores = {}\n",
    "    if city in city_routes:\n",
    "        route_graph = city_graphs[route_type]\n",
    "        num_targets = len(RouteType)\n",
    "\n",
    "        torch_geometric_data = get_torch_data(graph=route_graph, num_targets=num_targets)\n",
    "        num_nodes = len(route_graph.nodes())\n",
    "        # Torch data generated corresponds to an equivalent complete graph\n",
    "        num_edges = len(torch_geometric_data.edge_index[0])\n",
    "        torch_geometric_data = torch_geometric_data.to(device)\n",
    "\n",
    "        # Split train and test data\n",
    "        train_mask = np.zeros(num_edges, dtype=int)\n",
    "        train_mask[:int((1.000 - test_split) * num_edges)] = True\n",
    "        rng.shuffle(train_mask)\n",
    "        test_mask = ~train_mask\n",
    "\n",
    "        train_mask = torch.tensor(train_mask, device=device).bool()\n",
    "\n",
    "        train_data = Data(x=torch_geometric_data.x, edge_index=torch_geometric_data.edge_index[:, train_mask], y=torch_geometric_data.y[train_mask])\n",
    "        test_data  = Data(x=torch_geometric_data.x, edge_index=torch_geometric_data.edge_index[:, test_mask], y=torch_geometric_data.y[test_mask])\n",
    "\n",
    "        dataloader_train = DataLoader([train_data], shuffle=True)\n",
    "        dataloader_test = DataLoader([test_data], shuffle=True)\n",
    "\n",
    "        # GNN model for label prediction in edges\n",
    "        model = GNN(25, num_targets)\n",
    "        model = model.to(device)\n",
    "        loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "        # Train\n",
    "        train(model, loss_fn, device=device, optimizer=optimizer, max_epochs=50, train_dataloader=dataloader_train)\n",
    "\n",
    "        print(\"Training complete for city {}\".format(city))\n",
    "        print(\"Evaluating the model on testing data\")\n",
    "        # Evaluate the model\n",
    "        print(evaluate(model, loss_fcn=loss_fn, device=device, dataloader=dataloader_test))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 0.9589133858680725\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nitish/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, loss_fcn=loss_fn, device=device, dataloader=dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
