{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69533002-871d-4278-8d6a-2d385d2881a4",
   "metadata": {},
   "source": [
    "# Exploitation - 1 (Multilabel)\n",
    "\n",
    "In the exploitation task, we aim to predict the edges and the type of edges (transport routes). We start with Hand crafted features, followed by node embedding and finally use GNNs. In this notebook, we will work using the hand-crafted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc3f419-3674-4ab5-b866-7ecb3a9ede34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "mcolors = list(mcolors.BASE_COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecad3970-593b-474f-af83-2698d9de7bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rel_data_folder_path = pathlib.Path(\"./../../data\")\n",
    "transport_data_path = rel_data_folder_path.joinpath('transport_data')\n",
    "city_network = rel_data_folder_path.joinpath('network_graphs')\n",
    "city_network_graphs = city_network.joinpath('graphs')\n",
    "city_network_graphs_dir = city_network_graphs.joinpath('directed_graphs')\n",
    "# city_network_graphs_undir = city_network_graphs.joinpath('undirected_graphs')\n",
    "city_network_bones = city_network.joinpath('nodes-edges')\n",
    "\n",
    "checkpoints_folder_path = rel_data_folder_path.joinpath(\"checkpoints\")\n",
    "city_network_graphs_dir_betweenness = checkpoints_folder_path.joinpath('directed_graphs_betweenness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ebd660-a5c4-4e21-a5fe-d90172181c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define enum for route types\n",
    "class RouteType(Enum):\n",
    "    tram, subway, rail, bus, ferry, cablecar, gondola = range(7)\n",
    "\n",
    "def load_city_graphs(city_name, graphs_folder):\n",
    "    with open(graphs_folder.joinpath(city_name.lower() + '.gpickle'), 'rb') as f:\n",
    "        city_graph = pickle.load(f)\n",
    "    return city_graph\n",
    "\n",
    "def load_all_cities_graphs(cities: list[str], graphs_folder: pathlib.Path):\n",
    "    return {city: load_city_graphs(city, graphs_folder) for city in cities}\n",
    "\n",
    "def classifier_svm(features, targets, labels, feature_selection=False, num_features=1, test_size=0.5, seed=0, verbose=False, input_feature_names=None):\n",
    "    ''' \n",
    "    INPUT:\n",
    "    features: feature matrix of dimensions (N, D) (N: number of samples; D: number of features) \n",
    "    targets: target vector of dimensions (N, ) (N: number of samples)\n",
    "    labels: vector of all possible labels for nodes in the graph \n",
    "    feature_selection: a flag for whether to use feature selction, bool\n",
    "    num_features: number of features used from feature selesction\n",
    "    test_size: please set to 0.5 for reproducibility\n",
    "    seed: please set to 0 for reproducibility\n",
    "    verbose: print and plot result or not, bool\n",
    "    input_feature_names: Names for the input features, None by default\n",
    "\n",
    "    OUTPUT:\n",
    "    cm: confusion matrix on the test set\n",
    "    f1: weighted f1 score on the test set\n",
    "\n",
    "    ** Please set all of the random_state=seed in following module **\n",
    "    '''\n",
    "\n",
    "    # Split the data into training and testing sets, with test_size=0.5\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, stratify=targets, random_state=seed, test_size=test_size)\n",
    "\n",
    "    if feature_selection:\n",
    "        ## Build and train the ML model, including feature selection, normalization and Support Vector Classifier. Select the k highest relevant features for the classification. \n",
    "        feature_selector = SelectKBest(f_classif, k=num_features)\n",
    "        X_train_sel = feature_selector.fit_transform(X_train, y_train)\n",
    "        clf = make_pipeline(StandardScaler(), svm.SVC(class_weight='balanced', random_state=seed))\n",
    "        clf.fit(X_train_sel, y_train)\n",
    "        ## Print the scores for individual features.\n",
    "        print(\"\\nTo select the top {} features:\".format(num_features))\n",
    "        if not input_feature_names:\n",
    "            input_feature_names = [\"feature_{}\".format(str(i)) for i in range(features.shape[1])]\n",
    "        print(\"\\tThe feature scorees are:\")\n",
    "        print(\"\\t{}\".format({ft_name:ft_score for ft_name, ft_score in zip(input_feature_names, feature_selector.scores_)}))\n",
    "        print(\"\\tThe selected features are {}\".format(feature_selector.get_feature_names_out(input_features=input_feature_names)))\n",
    "    else:\n",
    "        ## Build and train the ML model, including normalization and Support Vector Classifier.\n",
    "        clf = make_pipeline(StandardScaler(), svm.SVC(class_weight='balanced', random_state=seed))\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model to predict the labels of the test data\n",
    "    if feature_selection:\n",
    "        X_test_sel = feature_selector.transform(X_test)\n",
    "        y_pred = clf.predict(X_test_sel)\n",
    "    else:\n",
    "        y_pred = clf.predict(X_test)\n",
    "    # Output the confusion matrix and weighted f1 score on the test set. Print the weighted f1 score and plot the confusion matrix if verbose\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    f1 = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    if verbose:\n",
    "        print(\"The Weighted F1 score is {}\".format(f1))\n",
    "        disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "    return cm, f1\n",
    "\n",
    "def classifier(features, targets, labels, feature_selection=False, num_features=1, test_size=0.5, seed=0, verbose=False, input_feature_names=None):\n",
    "    ''' \n",
    "    INPUT:\n",
    "    features: feature matrix of dimensions (N, D) (N: number of samples; D: number of features) \n",
    "    targets: target vector of dimensions (N, ) (N: number of samples)\n",
    "    labels: vector of all possible labels for nodes in the graph \n",
    "    feature_selection: a flag for whether to use feature selction, bool\n",
    "    num_features: number of features used from feature selesction\n",
    "    test_size: please set to 0.5 for reproducibility\n",
    "    seed: please set to 0 for reproducibility\n",
    "    verbose: print and plot result or not, bool\n",
    "    input_feature_names: Names for the input features, None by default\n",
    "\n",
    "    OUTPUT:\n",
    "    cm: confusion matrix on the test set\n",
    "    f1: weighted f1 score on the test set\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Split the data into training and testing sets, with test_size=0.5\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, stratify=targets, random_state=seed, test_size=test_size)\n",
    "\n",
    "    if feature_selection:\n",
    "        ## Build and train the ML model, including feature selection, normalization and Support Vector Classifier. Select the k highest relevant features for the classification. \n",
    "        feature_selector = SelectKBest(f_classif, k=num_features)\n",
    "        X_train_sel = feature_selector.fit_transform(X_train, y_train)\n",
    "        clf = make_pipeline(StandardScaler(), OneVsRestClassifier(LogisticRegression(class_weight='balanced', random_state=seed)))\n",
    "        clf.fit(X_train_sel, y_train)\n",
    "        ## Print the scores for individual features.\n",
    "        print(\"\\nTo select the top {} features:\".format(num_features))\n",
    "        if not input_feature_names:\n",
    "            input_feature_names = [\"feature_{}\".format(str(i)) for i in range(features.shape[1])]\n",
    "        print(\"\\tThe feature scorees are:\")\n",
    "        print(\"\\t{}\".format({ft_name:ft_score for ft_name, ft_score in zip(input_feature_names, feature_selector.scores_)}))\n",
    "        print(\"\\tThe selected features are {}\".format(feature_selector.get_feature_names_out(input_features=input_feature_names)))\n",
    "    else:\n",
    "        ## Build and train the ML model, including normalization and Logi.\n",
    "        clf = make_pipeline(StandardScaler(), OneVsRestClassifier(LogisticRegression(class_weight='balanced', random_state=seed)))\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model to predict the labels of the test data\n",
    "    if feature_selection:\n",
    "        X_test_sel = feature_selector.transform(X_test)\n",
    "        y_pred = clf.predict(X_test_sel)\n",
    "    else:\n",
    "        y_pred = clf.predict(X_test)\n",
    "    # Output the confusion matrix and weighted f1 score on the test set. Print the weighted f1 score and plot the confusion matrix if verbose\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    f1 = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    if verbose:\n",
    "        print(\"The Weighted F1 score is {}\".format(f1))\n",
    "        disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "    return cm, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d294700-0181-4f0a-ae6c-4610b7c06554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cities = sorted([x.stem for x in city_network_graphs_dir.glob('*.gpickle')])\n",
    "cities_graphs_dir = load_all_cities_graphs(cities, city_network_graphs_dir_betweenness)\n",
    "# cities_graphs_undir = load_all_cities_graphs(cities, city_network_graphs_undir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "240ff400-a4e4-4079-9213-62a3d6b9a4d7",
   "metadata": {},
   "source": [
    "## Task - 2\n",
    "\n",
    "In the second task, we predict the edges between the given nodes considering the full network. \n",
    "\n",
    "### Hand-crafted features\n",
    "\n",
    "For this task we consider the following node features\n",
    "\n",
    "1. In degree centrality\n",
    "2. Out degree centrality\n",
    "3. Betweenness centrality\n",
    "4. Katz centrality\n",
    "5. Clustering coefficient\n",
    "\n",
    "and the following edge features\n",
    "1. fraction of common predecessors among all predecessors\n",
    "2. fraction of common successors among all successors\n",
    "\n",
    "To compute the edge features between nodes, we combine the node features by subtracting the source node features from the target node features and then add the edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a10fac-eaab-4c18-b342-14959e34b503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_edge_feature_targets(graph: nx.Graph) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate a node feature matrix for a given graph.\n",
    "\n",
    "    Args:\n",
    "        graph: The input graph.\n",
    "\n",
    "    Returns:\n",
    "        edge_features: The edge features.\n",
    "        targes: target vector with values of {0, 1}\n",
    "        labels: vector of all possible targets in the graph \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    nodes = list(graph.nodes())\n",
    "    num_nodes = len(nodes)\n",
    "    \n",
    "    edge_features = []\n",
    "    targets = []\n",
    "    labels = []\n",
    "    \n",
    "    in_degree_centrality = nx.in_degree_centrality(graph)\n",
    "    out_degree_centrality = nx.out_degree_centrality(graph)\n",
    "    betweenness_centrality = nx.get_node_attributes(graph, \"betweenness\")\n",
    "    # katz_centrality = nx.katz_centrality(graph)\n",
    "    # clustering_coefficient = nx.clustering(graph)\n",
    "    \n",
    "    \n",
    "    for idx1 in tqdm(range(num_nodes)):\n",
    "        node1 = nodes[idx1]\n",
    "        # Get the predecessors (incoming neighbors) of the source node\n",
    "        source_predecessors = set(graph.predecessors(node1))\n",
    "        # Get the successors (outgoing neighbors) of the source node\n",
    "        source_successors = set(graph.successors(node1))\n",
    "        node_1_feat = [\n",
    "            in_degree_centrality[node1],\n",
    "            out_degree_centrality[node1], \n",
    "            betweenness_centrality[node1]\n",
    "            # katz_centrality[node1],\n",
    "            # clustering_coefficient[node1]\n",
    "        ]\n",
    "        for idx2 in range(num_nodes):\n",
    "            node2 = nodes[idx2]\n",
    "            node_2_feat = [\n",
    "                in_degree_centrality[node2],\n",
    "                out_degree_centrality[node2], \n",
    "                betweenness_centrality[node2]\n",
    "                # katz_centrality[node2],\n",
    "                # clustering_coefficient[node2]\n",
    "            ]\n",
    "            \n",
    "            comb_feature = np.subtract(node_2_feat, node_1_feat)\n",
    "            # avg_feature = np.mean(np.array([node_1_feat, node_2_feat ]), axis=0)\n",
    "            \n",
    "            if idx1 == idx2:\n",
    "                comm_preds = 1\n",
    "                comm_succe = 1\n",
    "            else:\n",
    "                # Get the predecessors (incoming neighbors) of the target node\n",
    "                target_predecessors = set(graph.predecessors(node2))\n",
    "\n",
    "                # Get the successors (outgoing neighbors) of the target node\n",
    "                target_successors = set(graph.successors(node2))\n",
    "                \n",
    "                union_predecessors = len(source_predecessors.union(target_predecessors))\n",
    "                union_successors = len(source_successors.union(target_successors))\n",
    "                \n",
    "                if union_predecessors == 0:\n",
    "                    comm_preds = 0\n",
    "                else:\n",
    "                    comm_preds = len(source_predecessors.intersection(target_predecessors)) / union_predecessors\n",
    "                \n",
    "                if union_successors == 0:\n",
    "                    comm_succe = 0\n",
    "                else:\n",
    "                    comm_succe = len(source_successors.intersection(target_successors)) / union_successors\n",
    "            \n",
    "            \n",
    "            edge_feature = np.concatenate((comb_feature, np.array([comm_preds, comm_succe])))\n",
    "            \n",
    "            edge_features.append(edge_feature)\n",
    "            target_multi = np.zeros(5)\n",
    "            if graph.number_of_edges(node1, node2) != 0:\n",
    "                for edg in graph.edges([node1, node2], data=True):\n",
    "                    target_multi[edg[2][\"route_type\"]] = 1\n",
    "            targets.append(target_multi)\n",
    "            \n",
    "    return np.array(edge_features), np.array(targets), np.unique(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed14c5f-3c7c-412a-8d14-131978ba026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Adelaide city\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'route_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFor \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m city\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(city\u001b[39m.\u001b[39mtitle()))\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m route_type, route_graph \u001b[39min\u001b[39;00m city_graphs\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mif\u001b[39;00m (route_label \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     12\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mFor \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m route\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(route\u001b[39m.\u001b[39mtitle()))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'route_label' is not defined"
     ]
    }
   ],
   "source": [
    "hand_feat_names = [\"in-deg\", \"out-deg\", \"betweeness\", \"katz\", \"clustering\", \"comm-pred\", \"comm-succ\"]\n",
    "ks = np.arange(1,len(hand_feat_names) + 1)\n",
    "\n",
    "\n",
    "cities_graphs_scores = {}\n",
    "\n",
    "for city, city_graphs in tqdm(cities_graphs_dir.items()):\n",
    "    cities_graphs_scores[city] = {}\n",
    "    print(\"For {} city\".format(city.title()))\n",
    "    for route_type, route_graph in city_graphs.items():\n",
    "        if (route_label != \"full\"):\n",
    "            continue\n",
    "        \n",
    "        print(\"\\tFor {} route\".format(route.title()))\n",
    "        \n",
    "        features_edges, targets_edge, labels_edge = generate_edge_feature_targets(route_graph)\n",
    "        \n",
    "        f1_ks = []\n",
    "        cm_ks = []\n",
    "        for k in ks:\n",
    "            cm, f1score = classifier(features_edges, targets_edge, labels_edge,\n",
    "                                  feature_selection=True, num_features=k,\n",
    "                                  test_size=0.5, seed=0, verbose=False,\n",
    "                                  input_feature_names=hand_feat_names)\n",
    "            print(\"F1 {}\".format(f1score))\n",
    "            f1_ks.append(f1score)\n",
    "            cm_ks.append(cm)            \n",
    "        cities_graphs_scores[city][route] = {\"f1\": f1_ks, \"cm\": cm_ks}\n",
    "            \n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(ks, f1_ks, label='F1 score')\n",
    "        ax.set_title(\"Weighted f1 score at different number of best K features\")\n",
    "        ax.set_xlabel(\"K\")\n",
    "        ax.set_ylabel(\"F1 score\");\n",
    "        plt.show()\n",
    "\n",
    "        k_best = np.argmax(f1_ks) + 1 # add one as the indexing starts from 0\n",
    "        print(\"\\t\\tThe best K with respect to weighted f1 score is: {}\".format(k_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e46cbb-f937-484e-912a-5b4cf8bb273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(checkpoints_folder_path.joinpath('full-multi-clasf-results.gpickle'), 'wb') as f:\n",
    "    pickle.dump(cities_graphs_scores, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
